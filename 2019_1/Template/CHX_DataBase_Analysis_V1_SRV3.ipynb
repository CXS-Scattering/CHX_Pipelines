{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header for 2018-1 kernel\n",
    "from pyCHX.chx_packages import *\n",
    "%matplotlib notebook\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.rcParams.update({ 'image.origin': 'lower'   })\n",
    "plt.rcParams.update({ 'image.interpolation': 'none'   })\n",
    "import pickle as cpk\n",
    "#from pyCHX.chx_xpcs_xsvs_jupyter_V1 import *\n",
    "#%run /home/yuzhang/pyCHX_link/pyCHX/chx_generic_functions.py\n",
    "#%matplotlib inline\n",
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyCHX.XPCS_SAXS import get_QrQw_From_RoiMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available databases:\n",
      "['amostra', 'chx-simulation-assetstore', 'chx-simulation-metadatastore', 'datastore', 'filestore', 'local', 'metadatastore', 'metadatastore-production-v1', 'samples']\n",
      "\n",
      " available collection in database samples:\n",
      "['samples', 'data_acquisition_collection', 'samples_2', 'debug', 'beamline_pos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda_envs/analysis-2019-1.2-chx/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: database_names is deprecated. Use list_database_names instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/conda_envs/analysis-2019-1.2-chx/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: collection_names is deprecated. Use list_collection_names instead.\n"
     ]
    }
   ],
   "source": [
    "# import database -> should be hidden from user in same package....\n",
    "import datetime\n",
    "import pymongo \n",
    "from tqdm import tqdm\n",
    "from bson import ObjectId\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.display import clear_output\n",
    "cli = pymongo.MongoClient('xf11id-ca')    \n",
    "samples_2 = cli.get_database('samples').get_collection('samples_2')\n",
    "data_acquisition_collection = cli.get_database('samples').get_collection('data_acquisition_collection')\n",
    "beamline_pos = cli.get_database('samples').get_collection('beamline_pos')\n",
    "from databroker import Broker                                                   \n",
    "db = Broker.named('temp')  # for real applications, 'temp' would be 'chx' \n",
    "print('available databases:')\n",
    "print(cli.database_names())\n",
    "print('\\n available collection in database samples:')\n",
    "print(cli.samples.collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chx_analysis_data( uid,  \n",
    "        #template_pipeline = '/home/yuzhang/analysis/2019_1/commisionning/XPCS_Single_2019_V1_SQRange.ipynb', \n",
    "        template_pipeline = '/home/yuzhang/analysis/2019_1/petrash/XPCS_Single_2019_V2.ipynb',                \n",
    "        outDir = '/home/yuzhang/analysis/2019_1/petrash/ResPipelines/',\n",
    "                      ):\n",
    "    ''' YG. Octo 6, 2018, Compress a eiger data using papermail\n",
    "    Input:\n",
    "        uid: string, the uique data id\n",
    "        force compress: if True, will force to compress data no matter the data was compressed already\n",
    "        The default compress pipeline  \n",
    "        template_pipeline: str, the filename of the template pipeline\n",
    "        outDir:str, the path for the output pipeline\n",
    "    Output:\n",
    "        save the current pipeline to outDir\n",
    "    '''\n",
    "    output_pipeline = outDir +   template_pipeline.split('/')[-1] + '_%s.ipynb'%uid \n",
    "    pm.execute_notebook(\n",
    "        template_pipeline, output_pipeline,         \n",
    "        parameters = dict( uid = uid,  ),\n",
    "                    kernel_name= None, report_mode=True )      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "if True:    \n",
    "    temp1 = data_acquisition_collection.find_one({'_id':'general_list'})['compression_completed']\n",
    "    temp2= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_completed']\n",
    "    #temp3= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_failed']\n",
    "    temp4= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_failed_userX']\n",
    "    s2 = set(temp2)\n",
    "    #s3 = set(temp3)\n",
    "    s4 = set(temp4)       \n",
    "    #######################################\n",
    "    #######Get uids to be compressed#######    \n",
    "    uids = [x for x in temp1 if x not in s2 and x not in s4] \n",
    "    print(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_analysis_database( start_uid ):\n",
    "    '''Give the uid for the first running and get the masked database'''\n",
    "    temp1 = data_acquisition_collection.find_one({'_id':'general_list'})['compression_completed']\n",
    "    for i,  t in enumerate(temp1):\n",
    "        if t == start_uid:\n",
    "            print(i,t)\n",
    "            start_id = i    \n",
    "    masked = data_acquisition_collection.update_one( \n",
    "           {'_id':'general_list'},{'$set':{'analysis_failed_userX':  temp1[:start_id] }   })\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_masked_analysis_database( start_uid = 'b2dbbb62-633e-45af-ac33-d64bb3fdd373' )\n",
    "#get_masked_analysis_database( start_uid ='40920a78-750f-4812-a604-601d2e6677dc')\n",
    "#get_masked_analysis_database( start_uid ='70a31366-045e-40ad-8d18-281a9b8729a2')\n",
    "#get_masked_analysis_database( start_uid ='e2d0ae81-ee8d-4b6c-9f32-aefc50b09b9a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_acquisition_collection.update_one( {'_id':'general_list'},{'$set':{'analysis_failed_petrash':  temp1[:797] }   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp4.append(['2891d947-58b1-4db8-9b7e-c93f94259e78',   'f34aceea-dacb-439f-9e91-94bc37156354',          '0639c0c3-7257-436f-9bfe-b964830e3823']   )\n",
    "#data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{'analysis_failed_Surita':tem}})\n",
    "#data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{'analysis_failed':[]}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faking data analysis from compressed_uid list in data-acquisition database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_acquisition_collection.update_one( {'_id':'general_list'}, {'$set':{'analysis_in_progress': [] }}) \n",
    "#data_acquisition_collection.find_one({'_id':'general_list'})['analysis_in_progress']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of uids for analysis is emtpy...going to look again in 5s.\n"
     ]
    }
   ],
   "source": [
    "# get list of uids\n",
    "end_of_compression_key='none' # stops if only this key is left in compressed_uid_list, 'none': not looking for key, just for empty list timeout\n",
    "empty_list_timeout= 3600 * 24 * 7 #[s] stops if compressed_uid_list is empty for x s\n",
    "\n",
    "time_count=0\n",
    "run_condition = True\n",
    "while run_condition:\n",
    "    clear_output()\n",
    "    temp1 = data_acquisition_collection.find_one({'_id':'general_list'})['compression_completed']\n",
    "    temp2= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_completed']\n",
    "    temp3= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_failed_userX']\n",
    "    temp4 = data_acquisition_collection.find_one({'_id':'general_list'})['analysis_in_progress']\n",
    "    #data_acquisition_collection.find_one({'_id':'general_list'})['analysis_failed']\n",
    "    s2 = set(temp2)\n",
    "    s3 = set(temp3)\n",
    "    s4 = set( temp4 )   \n",
    "    #######################################\n",
    "    #######Get uids to be compressed#######    \n",
    "    uids = [x for x in temp1 if x not in s2 and x not in s3] \n",
    "    ######################################\n",
    "    N = len(uids)\n",
    "    if N>=3: # list of uids is not empty\n",
    "        print('uid list for analysis is NOT empty, found '+str(len(uids))+' uids awaiting analysis.')\n",
    "        time_count=0\n",
    "        if end_of_compression_key != 'none' and uids[-1] == end_of_compression_key: #looking for a stop key, next uid up IS the stop key\n",
    "            run_condition = False\n",
    "            print('Stop Key for analysis detected!')\n",
    "            \n",
    "        else:\n",
    "            print('Doing data analysis for uid '+uids[-1])\n",
    "            \n",
    "            #######################################\n",
    "            #for ics in tqdm(range(100)):\n",
    "            #    time.sleep(.35)\n",
    "            ########################################\n",
    "            uid = uids[-1]\n",
    "            if uid not in s4:\n",
    "                try:                    \n",
    "                    temp4.append( uid )\n",
    "                    data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{ 'analysis_in_progress': temp4}})\n",
    "                     \n",
    "                    _chx_analysis_data( uid )                 \n",
    "                        # update list of compressed uids:\n",
    "                    temp2= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_completed']     \n",
    "                    temp2.append(   uid  )\n",
    "                    data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{ 'analysis_completed': temp2}})\n",
    "                   \n",
    "                except:\n",
    "                    temp3= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_failed_userX']\n",
    "                    temp3.append(   uid )\n",
    "                    data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{ 'analysis_failed_userX': temp3}})\n",
    "                \n",
    "                ##remove this uid from  uid in analysis_in_progress\n",
    "                temp4 = data_acquisition_collection.find_one({'_id':'general_list'})['analysis_in_progress']\n",
    "                tx = [u for u in temp4  if u!=uid ]\n",
    "                data_acquisition_collection.update_one( {'_id':'general_list'},\n",
    "                                                           {'$set':{'analysis_in_progress':  tx }   }) \n",
    "\n",
    "                \n",
    "    else:\n",
    "        if time_count > empty_list_timeout:\n",
    "            print('uid list for analysis was empty for > '+str(empty_list_timeout)+'s -> stop looking for new uids')\n",
    "            run_condition = False\n",
    "        else:\n",
    "            time_count=time_count+5\n",
    "            print('list of uids for analysis is emtpy...going to look again in 5s.')\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
