{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header for 2018-1 kernel\n",
    "from pyCHX.chx_packages import *\n",
    "%matplotlib notebook\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.rcParams.update({ 'image.origin': 'lower'   })\n",
    "plt.rcParams.update({ 'image.interpolation': 'none'   })\n",
    "import pickle as cpk\n",
    "#from pyCHX.chx_xpcs_xsvs_jupyter_V1 import *\n",
    "#%run /home/yuzhang/pyCHX_link/pyCHX/chx_generic_functions.py\n",
    "#%matplotlib inline\n",
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available databases:\n",
      "['amostra', 'chx-simulation-assetstore', 'chx-simulation-metadatastore', 'datastore', 'filestore', 'local', 'metadatastore', 'metadatastore-production-v1', 'samples']\n",
      "\n",
      " available collection in database samples:\n",
      "['samples', 'data_acquisition_collection', 'samples_2', 'debug', 'beamline_pos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda_envs/analysis-2019-1.2-chx/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: database_names is deprecated. Use list_database_names instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/conda_envs/analysis-2019-1.2-chx/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: collection_names is deprecated. Use list_collection_names instead.\n"
     ]
    }
   ],
   "source": [
    "# import database -> should be hidden from user in same package....\n",
    "import datetime\n",
    "import pymongo \n",
    "from tqdm import tqdm\n",
    "from bson import ObjectId\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.display import clear_output\n",
    "cli = pymongo.MongoClient('xf11id-ca')    \n",
    "samples_2 = cli.get_database('samples').get_collection('samples_2')\n",
    "data_acquisition_collection = cli.get_database('samples').get_collection('data_acquisition_collection')\n",
    "beamline_pos = cli.get_database('samples').get_collection('beamline_pos')\n",
    "from databroker import Broker                                                   \n",
    "db = Broker.named('temp')  # for real applications, 'temp' would be 'chx' \n",
    "print('available databases:')\n",
    "print(cli.database_names())\n",
    "print('\\n available collection in database samples:')\n",
    "print(cli.samples.collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chx_analysis_data( uid,  \n",
    "        #template_pipeline = '/home/yuzhang/analysis/2019_1/commisionning/XPCS_Single_2019_V1_SQRange.ipynb', \n",
    "        template_pipeline = '/home/yuzhang/analysis/2019_1/petrash/XPCS_Single_2019_V2.ipynb',                \n",
    "        outDir = '/home/yuzhang/analysis/2019_1/petrash/ResPipelines/',\n",
    "                      ):\n",
    "    ''' YG. Octo 6, 2018, Compress a eiger data using papermail\n",
    "    Input:\n",
    "        uid: string, the uique data id\n",
    "        force compress: if True, will force to compress data no matter the data was compressed already\n",
    "        The default compress pipeline  \n",
    "        template_pipeline: str, the filename of the template pipeline\n",
    "        outDir:str, the path for the output pipeline\n",
    "    Output:\n",
    "        save the current pipeline to outDir\n",
    "    '''\n",
    "    output_pipeline = outDir +   template_pipeline.split('/')[-1] + '_%s.ipynb'%uid \n",
    "    pm.execute_notebook(\n",
    "        template_pipeline, output_pipeline,         \n",
    "           parameters = dict( uid = uid,  ),\n",
    "            kernel_name='python3', report_mode=True )      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de9bd5ba-897d-4719-87a3-c5ad6ebf50c1', '396b8e68-acd2-4b21-8af7-bb3a8685549b', 'f3f39aec-db19-4c77-9cb4-4458261714df']\n"
     ]
    }
   ],
   "source": [
    "if True:    \n",
    "    temp1 = data_acquisition_collection.find_one({'_id':'general_list'})['compression_completed']\n",
    "    temp2= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_completed']\n",
    "    #temp3= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_failed']\n",
    "    temp4= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_failed_userX']\n",
    "    s2 = set(temp2)\n",
    "    #s3 = set(temp3)\n",
    "    s4 = set(temp4)       \n",
    "    #######################################\n",
    "    #######Get uids to be compressed#######    \n",
    "    uids = [x for x in temp1 if x not in s2 and x not in s4] \n",
    "    print(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp1[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp4[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp2[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not_done=[x for x in temp1 if x not in s2 and x not in s4]\n",
    "#not_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f3f39aec-db19-4c77-9cb4-4458261714df',\n",
       " 'd9fc1e0e-a5b9-4fe9-995f-a8f6fff370e3',\n",
       " '2494fe5e-6d1a-4c18-805a-57c6e7d34acc')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1[-1], temp2[-1], temp4[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_analysis_database( start_uid ):\n",
    "    '''Give the uid for the first running and get the masked database'''\n",
    "    temp1 = data_acquisition_collection.find_one({'_id':'general_list'})['compression_completed']\n",
    "    for i,  t in enumerate(temp1):\n",
    "        if t == start_uid:\n",
    "            print(i,t)\n",
    "            start_id = i    \n",
    "    masked = data_acquisition_collection.update_one( \n",
    "           {'_id':'general_list'},{'$set':{'analysis_failed_userX':  temp1[:start_id] }   })\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816 2494fe5e-6d1a-4c18-805a-57c6e7d34acc\n"
     ]
    }
   ],
   "source": [
    "start_fuid = '2494fe5e-6d1a-4c18-805a-57c6e7d34acc' #'25171c35-ce50-450b-85a0-ba9e116651e3'\n",
    "get_masked_analysis_database( start_uid = start_fuid )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7f213a4dbf88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_acquisition_collection.update_one( {'_id':'general_list'},{'$set':{'analysis_failed_petrash':  temp1[:797] }   })\n",
    "data_acquisition_collection.update_one( {'_id':'general_list'},{'$set':{'analysis_in_progress':  [] }   })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_acquisition_collection.find_one({'_id':'general_list'})['analysis_in_progress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp4.append(['2891d947-58b1-4db8-9b7e-c93f94259e78',   'f34aceea-dacb-439f-9e91-94bc37156354',          '0639c0c3-7257-436f-9bfe-b964830e3823']   )\n",
    "#data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{'analysis_failed_Surita':tem}})\n",
    "#data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{'analysis_failed':[]}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faking data analysis from compressed_uid list in data-acquisition database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get list of uids\n",
    "end_of_compression_key='none' # stops if only this key is left in compressed_uid_list, 'none': not looking for key, just for empty list timeout\n",
    "empty_list_timeout= 3600 * 24 * 7  #[s] stops if compressed_uid_list is empty for x s\n",
    "\n",
    "time_count=0\n",
    "run_condition = True\n",
    "while run_condition:\n",
    "    clear_output()\n",
    "    temp1 = data_acquisition_collection.find_one({'_id':'general_list'})['compression_completed']\n",
    "    temp2= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_completed']\n",
    "    temp3= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_failed_userX']\n",
    "    temp4 = data_acquisition_collection.find_one({'_id':'general_list'})['analysis_in_progress']\n",
    "    #data_acquisition_collection.find_one({'_id':'general_list'})['analysis_failed']\n",
    "    s2 = set(temp2)\n",
    "    s3 = set(temp3)\n",
    "    s4 = set( temp4 )\n",
    "       \n",
    "    #######################################\n",
    "    #######Get uids to be compressed#######    \n",
    "    uids = [x for x in temp1 if x not in s2 and x not in s3] \n",
    "    ######################################\n",
    "    \n",
    "    \n",
    "    if uids: # list of uids is not empty\n",
    "        print('uid list for analysis is NOT empty, found '+str(len(uids))+' uids awaiting analysis.')\n",
    "        time_count=0\n",
    "        if end_of_compression_key != 'none' and uids[0] == end_of_compression_key: #looking for a stop key, next uid up IS the stop key\n",
    "            run_condition = False\n",
    "            print('Stop Key for analysis detected!')\n",
    "            \n",
    "        else:\n",
    "            print('Doing data analysis for uid '+uids[0])\n",
    "            \n",
    "            #######################################\n",
    "            #for ics in tqdm(range(100)):\n",
    "            #    time.sleep(.35)\n",
    "            ########################################\n",
    "            uid = uids[0]\n",
    "            if uid not in s4:\n",
    "                try:                                        \n",
    "                    temp4.append( uid )\n",
    "                    data_acquisition_collection.update_one({'_id':'general_list'},\n",
    "                                                         {'$set':{ 'analysis_in_progress': temp4}}) \n",
    "\n",
    "                    _chx_analysis_data( uid )                 \n",
    "                        # update list of compressed uids:\n",
    "                    temp2= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_completed']    \n",
    "                    temp2.append(uids[0])\n",
    "                    data_acquisition_collection.update_one({'_id':'general_list'},\n",
    "                                                    {'$set':{ 'analysis_completed': temp2}})            \n",
    "                \n",
    "                except:\n",
    "                    temp3= data_acquisition_collection.find_one({'_id':'general_list'})['analysis_failed_userX']\n",
    "                    temp3.append(uids[0])\n",
    "                    data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{ 'analysis_failed_userX': temp3}})\n",
    "                    \n",
    "                ##remove this uid from  uid in analysis_in_progress\n",
    "                temp4 = data_acquisition_collection.find_one({'_id':'general_list'})['analysis_in_progress']\n",
    "                tx = [u for u in temp4  if u!=uid ]\n",
    "                data_acquisition_collection.update_one( {'_id':'general_list'},\n",
    "                                                           {'$set':{'analysis_in_progress':  tx }   })                     \n",
    "\n",
    "\n",
    "                \n",
    "    else:\n",
    "        if time_count > empty_list_timeout:\n",
    "            print('uid list for analysis was empty for > '+str(empty_list_timeout)+'s -> stop looking for new uids')\n",
    "            run_condition = False\n",
    "        else:\n",
    "            time_count=time_count+5\n",
    "            print('list of uids for analysis is emtpy...going to look again in 5s.')\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
