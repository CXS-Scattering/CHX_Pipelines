{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header for 2018-1 kernel\n",
    "from pyCHX.chx_packages import *\n",
    "%matplotlib notebook\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.rcParams.update({ 'image.origin': 'lower'   })\n",
    "plt.rcParams.update({ 'image.interpolation': 'none'   })\n",
    "import pickle as cpk\n",
    "from pyCHX.chx_xpcs_xsvs_jupyter_V1 import *\n",
    "#%run /home/yuzhang/pyCHX_link/pyCHX/chx_generic_functions.py\n",
    "import papermill as pm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available databases:\n",
      "['amostra', 'chx-simulation-assetstore', 'chx-simulation-metadatastore', 'datastore', 'filestore', 'local', 'metadatastore', 'metadatastore-production-v1', 'samples']\n",
      "\n",
      " available collection in database samples:\n",
      "['samples', 'data_acquisition_collection', 'samples_2', 'debug', 'beamline_pos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda_envs/analysis-2019-3.0-chx/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: database_names is deprecated. Use list_database_names instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/conda_envs/analysis-2019-3.0-chx/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: collection_names is deprecated. Use list_collection_names instead.\n"
     ]
    }
   ],
   "source": [
    "# import database -> should be hidden from user in same package....\n",
    "import datetime\n",
    "import pymongo \n",
    "from tqdm import tqdm\n",
    "from bson import ObjectId\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.display import clear_output\n",
    "cli = pymongo.MongoClient('xf11id-ca')    \n",
    "samples_2 = cli.get_database('samples').get_collection('samples_2')\n",
    "data_acquisition_collection = cli.get_database('samples').get_collection('data_acquisition_collection')\n",
    "beamline_pos = cli.get_database('samples').get_collection('beamline_pos')\n",
    "from databroker import Broker                                                   \n",
    "dbt = Broker.named('temp')  # for real applications, 'temp' would be 'chx' \n",
    "print('available databases:')\n",
    "print(cli.database_names())\n",
    "print('\\n available collection in database samples:')\n",
    "print(cli.samples.collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uid = '2555d366' #] (scan num: 24810) (Measurement: Test images from reference 8CB planar cell detectorx=154.9712,detectory=-132.6920 T=33.9C, real T=33.1 )\n",
    "def _chx_compress_data( uid, force_compress = False,\n",
    "                      template_pipeline = '/nsls2/xf11id1/analysis/Template_Pipelines/2019/Template_CHX_CompressData_V0.ipynb',               \n",
    "                      outDir =  '/nsls2/xf11id1/analysis/temp/',\n",
    "                       nobytes=4,\n",
    "                       \n",
    "                      ):\n",
    "    ''' YG. Octo 6, 2018, Compress a eiger data using papermail\n",
    "    Input:\n",
    "        uid: string, the uique data id\n",
    "        force compress: if True, will force to compress data no matter the data was compressed already\n",
    "        The default compress pipeline  \n",
    "        template_pipeline: str, the filename of the template pipeline\n",
    "        outDir:str, the path for the output pipeline\n",
    "    Output:\n",
    "        save the current pipeline into a output_pipeline\n",
    "    '''\n",
    "    output_pipeline = outDir +  'CHX_CompressData_V0.ipynb'\n",
    "    pm.execute_notebook(\n",
    "        template_pipeline, output_pipeline,         \n",
    "        parameters = dict( uid = uid, force_compress=force_compress, nobytes= nobytes),\n",
    "                    kernel_name= None, report_mode=True )   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    temp1 = data_acquisition_collection.find_one({'_id':'general_list'})['uid_list']\n",
    "    temp2= data_acquisition_collection.find_one({'_id':'general_list'})['compression_completed']\n",
    "    temp3= data_acquisition_collection.find_one({'_id':'general_list'})['compression_failed_X']    \n",
    "    #data_acquisition_collection.find_one({'_id':'general_list'})['compression_failed']\n",
    "    s2 = set(temp2)\n",
    "    s3 = set(temp3)\n",
    "    uids = [x for x in temp1 if x not in s2 and x not in s3]\n",
    "    #uids = [x for x in temp1 if x not in s2 and x not in s3]\n",
    "    print(uids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{'compression_failed_X': temp1[:-10] }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['72ad7667-30cf-4630-8aa0-dcac2cec333f',\n",
       " 'b3a854be-a523-443a-bf66-fc2ee22ef32a',\n",
       " 'fbf1aab9-3096-468e-b2ec-ff8ae712e22e']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['40589807-a18b-4186-8d88-d4262af3d999',\n",
       " '72ad7667-30cf-4630-8aa0-dcac2cec333f',\n",
       " 'fbf1aab9-3096-468e-b2ec-ff8ae712e22e']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['72ad7667-30cf-4630-8aa0-dcac2cec333f',\n",
       " 'b3a854be-a523-443a-bf66-fc2ee22ef32a',\n",
       " 'fbf1aab9-3096-468e-b2ec-ff8ae712e22e']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_compress_database( start_uid ):\n",
    "    '''Give the uid for the first running and get the masked database'''\n",
    "    #temp1 = data_acquisition_collection.find_one({'_id':'general_list'})['compression_completed']\n",
    "    temp1 = data_acquisition_collection.find_one({'_id':'general_list'})['uid_list']\n",
    "    for i,  t in enumerate(temp1):\n",
    "        if t == start_uid:\n",
    "            print(i,t)\n",
    "            start_id = i +1    \n",
    "    masked = data_acquisition_collection.update_one( \n",
    "           {'_id':'general_list'},{'$set':{'compression_failed_X':  temp1[:start_id] }   })\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_masked_compress_database( start_uid =  '76f84e1e-62e6-4251-812a-ef0f9c6d36b7' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b3a854be-a523-443a-bf66-fc2ee22ef32a',\n",
       " 'fbf1aab9-3096-468e-b2ec-ff8ae712e22e']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3= data_acquisition_collection.find_one({'_id':'general_list'})['compression_failed_X'] \n",
    "temp3[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_acquisition_collection.update_one( {'_id':'general_list'}, {'$set':{'compression_failed_X':  temp3[:-2] } })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    temp1 = data_acquisition_collection.find_one({'_id':'general_list'})['uid_list']\n",
    "    temp2= data_acquisition_collection.find_one({'_id':'general_list'})['compression_completed']\n",
    "    temp3= data_acquisition_collection.find_one({'_id':'general_list'})['compression_failed_X']    \n",
    "    #data_acquisition_collection.find_one({'_id':'general_list'})['compression_failed']\n",
    "    s2 = set(temp2)\n",
    "    s3 = set(temp3)\n",
    "    uids = [x for x in temp1 if x not in s2 and x not in s3]\n",
    "    #uids = [x for x in temp1 if x not in s2 and x not in s3]\n",
    "    print(uids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pymongo  \n",
    "#from IPython.display import clear_output\n",
    "#cli = pymongo.MongoClient('xf11id-ca')    \n",
    "#samples_2 = cli.get_database('samples').get_collection('samples_2')\n",
    "#data_acquisition_collection = cli.get_database('samples').get_collection('data_acquisition_collection')\n",
    "\n",
    "\n",
    "# How to find/list one entery\n",
    "#data_acquisition_collection.find_one({'_id':'general_list'})['uid_list']\n",
    "# How to find and delete one entery\n",
    "#samples_2.find_one_and_delete(    {'info.owner':'chx'}  )\n",
    "# How to create/update an entry\n",
    "#temp1=['uid1', 'uid2']\n",
    "#data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{ 'uid_list': temp1}})\n",
    "#data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{'analysis_failed':[]}})\n",
    "#data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{'compression_failed': temp1 }})\n",
    "\n",
    "# How to delete an entry\n",
    "#data_acquisition_collection.delete_one( {'_id':'general_list/compresion_completed'} )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_acquisition_collection.update_one( {'_id':'general_list'}, {'$set':{'compression_in_progress': [] }}) \n",
    "#data_acquisition_collection.find_one({'_id':'general_list'})['compression_in_progress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run /home/yuzhang/pyCHX_link/chx_generic_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCHX.chx_generic_functions import append_txtfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'commissioning'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nsls2/xf11id1/analysis/2019_3/commissioning/\n"
     ]
    }
   ],
   "source": [
    "txtDir = '/nsls2/xf11id1/analysis/2019_3/%s/'%username\n",
    "txt_filename = 'AutoCompression.txt'\n",
    "X = []\n",
    "txt_header = 'fuid,  cmp_time, comp_status'\n",
    "print(txtDir,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data compression from uid list in data-acquisition database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of uids for compression is emtpy...going to look again in 5s.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-05747d6a3203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtime_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_count\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'list of uids for compression is emtpy...going to look again in 5s.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get list of uids\n",
    "stop_key='none' # stops if only this key is left in uid_list, 'none': not looking for key, just for empty list timeout\n",
    "empty_list_timeout= 3600 * 24 * 8  #[s] stops if uid_list is empty for x s\n",
    "end_of_compression_key='none' # if not 'none': write key to list of compressed uids to mark end\n",
    "\n",
    "time_count=0\n",
    "run_condition = True\n",
    "while run_condition:\n",
    "    clear_output()\n",
    "    \n",
    "    temp1 = data_acquisition_collection.find_one({'_id':'general_list'})['uid_list']\n",
    "    temp2= data_acquisition_collection.find_one({'_id':'general_list'})['compression_completed']\n",
    "    temp3= data_acquisition_collection.find_one({'_id':'general_list'})['compression_failed_X']\n",
    "    #data_acquisition_collection.find_one({'_id':'general_list'})['compression_failed']\n",
    "    temp4 = data_acquisition_collection.find_one({'_id':'general_list'})['compression_in_progress']\n",
    "    s2 = set(temp2)\n",
    "    s3 = set(temp3)\n",
    "    s4 = set(temp4)\n",
    "       \n",
    "    #######################################\n",
    "    #######Get uids to be compressed#######    \n",
    "    uids = [x for x in temp1 if x not in s2 and x not in s3] \n",
    "    ######################################\n",
    "\n",
    "    if uids: # list of uids is not empty\n",
    "        print('uid list for compression is NOT empty, found '+str(len(uids))+' uids awaiting compression.')\n",
    "        time_count=0\n",
    "        if stop_key != 'none' and uids[0] == stop_key: #looking for a stop key, but the next uid up is not the stop key\n",
    "            run_condition = False\n",
    "            print('Stop Key for compression detected!')\n",
    "            \n",
    "        else:\n",
    "            print('Doing data file compression for uid '+uids[0])\n",
    "            \n",
    "            #############Here will update with a real code for compression  \n",
    "            #for ics in tqdm(range(100)):\n",
    "            #    time.sleep(.23)\n",
    "            uid = uids[0]    \n",
    "            if uid not in s4:\n",
    "                t0 = time.time()\n",
    "                try:\n",
    "                    \n",
    "                    nobytes=4\n",
    "                    detectors = sorted(get_detectors(db[uid])) \n",
    "                    for det in detectors:\n",
    "                        if '500' in det:\n",
    "                            nobytes=4 \n",
    "                    print('nobytes is: %s'%nobytes)        \n",
    "                            \n",
    "                            \n",
    "                    _chx_compress_data( uid, nobytes= nobytes )  \n",
    "                    # update list of compressed uids:\n",
    "                    temp2.append(uids[0])            \n",
    "                    data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{ 'compression_completed': temp2}})\n",
    "                    status='succeed'\n",
    "                except:\n",
    "                    print('This uid=%s can not be compressed.'%uid)\n",
    "                    # update list of failed compressed uids:\n",
    "                    temp3.append(uids[0])            \n",
    "                    data_acquisition_collection.update_one({'_id':'general_list'},{'$set':{ 'compression_failed_X': temp3}})                 \n",
    "                    status='fail'\n",
    "                ###################################### \n",
    "                \n",
    "                \n",
    "                ts = (time.time() - t0)/60 #in unit of min                \n",
    "                txt_header = 'fuid,  comp_time, comp_status'                \n",
    "                x =   [ uid,  ts, status  ]  \n",
    "                X.append(x)     \n",
    "                append_txtfile( txtDir + txt_filename, data=X, fmt='%s',\n",
    "                        delimiter=',', header= txt_header) \n",
    "                \n",
    "                \n",
    "                \n",
    "                ##remove this uid from  uid in analysis_in_progress\n",
    "                temp4 = data_acquisition_collection.find_one({'_id':'general_list'})['compression_in_progress']\n",
    "                tx = [u for u in temp4  if u!=uid ]\n",
    "                data_acquisition_collection.update_one( {'_id':'general_list'},\n",
    "                                                           {'$set':{'compression_in_progress':  tx }   })  \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "    else:\n",
    "        if time_count > empty_list_timeout:\n",
    "            print('uid list for compression was empty for > '+str(empty_list_timeout)+'s -> stop looking for new uids')\n",
    "            run_condition = False\n",
    "        else:\n",
    "            time_count=time_count+5\n",
    "            print('list of uids for compression is emtpy...going to look again in 5s.')\n",
    "            time.sleep(5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_acquisition_collection.find_one({'_id':'general_list'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db[uid]['start']['detectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
